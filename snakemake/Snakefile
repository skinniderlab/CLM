configfile: "config.json"
threads: 1
# -----------------------------------------------------------------------------
# Setup
# -----------------------------------------------------------------------------
DATASETS = config["datasets"]
REPRESENTATIONS = config["representations"]
SAMPLE_IDXS = config["sample_idxs"]
K = config["k"]
OUTPUT_DIR = config['output_dir']
MODEL_PARAMS = config['model_params']

shell.executable("/bin/bash")

wildcard_constraints:
    dataset="|".join(DATASETS)

# Rules that should be run on the head node in a cluster environment
# These typically involve web access or one time installation steps
localrules:
    download_datasets

# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Rules
# -----------------------------------------------------------------------------
rule all:
    input:
        smiles_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_SMILES.smi",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1)),
        model_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_model.pt",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1)),
        loss_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_loss.csv",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1)),
        time_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_timing.csv",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1))


rule download_datasets:
    params:
        url=lambda wildcards: config['urls'][wildcards.dataset]
    output:
        f"{OUTPUT_DIR}/prior/raw/{{dataset}}.txt"
    shell: """
        wget -O - {params.url} | gunzip -c > {output}
        """

rule generate_prior_inputs:
    input:
        f"{OUTPUT_DIR}/prior/raw/{{dataset}}.txt"
    output:
        train_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.csv",
        vocab_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.vocabulary",
        test_file=f"{OUTPUT_DIR}/prior/inputs/test_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.csv"
    shell: """
        python ../python/inner-preprocess-prior-datasets.py \
        --representation {wildcards.repr} \
        --enum_factor 0 \
        --sample_idx {wildcards.sample_idx} \
        --k {K} \
        --cv_fold {wildcards.fold} \
        --input_file {input} \
        --train_file {output.train_file} \
        --vocab_file {output.vocab_file} \
        --test_file {output.test_file}
        """


rule inner_train_models_RNN:
    input:
        input_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.csv",
        vocab_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.vocabulary"
    output:
        smiles_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_SMILES.smi",
        model_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_model.pt",
        loss_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_loss.csv",
        time_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}_timing.csv"
    shell:
            'python ../python/inner-train-models-RNN.py '
            '--database {wildcards.dataset} '
            '--representation {wildcards.repr} '
            '--enum_factor {MODEL_PARAMS[enum_factor]} '
            '--n_molecules {MODEL_PARAMS[n_molecules]} '
            '--min_tc {MODEL_PARAMS[min_tc]} '
            '--sample_idx {wildcards.sample_idx} '
            '--rnn_type {MODEL_PARAMS[rnn_type]} '
            '--embedding_size {MODEL_PARAMS[embedding_size]} '
            '--hidden_size {MODEL_PARAMS[hidden_size]} '
            '--n_layers {MODEL_PARAMS[n_layers]} '
            '--dropout {MODEL_PARAMS[dropout]} '
            '--batch_size {MODEL_PARAMS[batch_size]} '
            '--learning_rate {MODEL_PARAMS[learning_rate]} '
            '--max_epochs {MODEL_PARAMS[max_epochs]} '
            '--patience {MODEL_PARAMS[patience]} '
            '--log_every_steps {MODEL_PARAMS[log_every_steps]} '
            '--log_every_epochs {MODEL_PARAMS[log_every_epochs]} '
            '--sample_mols {MODEL_PARAMS[sample_mols]} '
            '--input_file {input.input_file} '
            '--vocab_file {input.vocab_file} '
            '--smiles_file {output.smiles_file} '
            '--model_file {output.model_file} '
            '--loss_file {output.loss_file} '
            '--time_file {output.time_file} '
# -----------------------------------------------------------------------------
