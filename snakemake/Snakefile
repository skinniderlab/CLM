configfile: "config.json"
threads: 1
# -----------------------------------------------------------------------------
# Setup
# -----------------------------------------------------------------------------
DATASETS = config["datasets"]
REPRESENTATIONS = config["representations"]
SAMPLE_IDXS = config["sample_idxs"]
K = config["k"]
OUTPUT_DIR = config['output_dir']

shell.executable("/bin/bash")

wildcard_constraints:
    dataset="|".join(DATASETS)

# Rules that should be run on the head node in a cluster environment
# These typically involve web access or one time installation steps
localrules:
    download_datasets

# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Rules
# -----------------------------------------------------------------------------
rule all:
    input:
        smiles_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/SMILES.smi",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1)),
        model_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/model.pt",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1)),
        loss_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/loss.csv",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1)),
        time_file = expand(f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/timing.csv",
                        dataset=DATASETS, repr=REPRESENTATIONS, sample_idx=SAMPLE_IDXS, fold=range(1, K + 1))


rule download_datasets:
    params:
        url=lambda wildcards: config['urls'][wildcards.dataset]
    output:
        f"{OUTPUT_DIR}/prior/raw/{{dataset}}.txt"
    shell: """
        wget -O - {params.url} | gunzip -c > {output}
        """

rule generate_prior_inputs:
    input:
        f"{OUTPUT_DIR}/prior/raw/{{dataset}}.txt"
    output:
        train_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.csv",
        vocab_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.vocabulary",
        test_file=f"{OUTPUT_DIR}/prior/inputs/test_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.csv"
    shell: """
        python ../python/inner-preprocess-prior-datasets.py \
        --representation {wildcards.repr} \
        --enum_factor 0 \
        --sample_idx {wildcards.sample_idx} \
        --k {K} \
        --cv_fold {wildcards.fold} \
        --input_file {input} \
        --train_file {output.train_file} \
        --vocab_file {output.vocab_file} \
        --test_file {output.test_file}
        """


rule inner_train_models_RNN:
    input:
        input_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.csv",
        vocab_file=f"{OUTPUT_DIR}/prior/inputs/train_{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}.vocabulary"
    output:
        smiles_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/SMILES.smi",
        model_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/model.pt",
        loss_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/loss.csv",
        time_file=f"{OUTPUT_DIR}/prior/models/{{dataset}}_{{repr}}_sample{{sample_idx}}_{{fold}}/timing.csv"
    shell:
            'python ../python/inner-train-models-RNN.py '
            '--database {wildcards.dataset} '
            '--representation {wildcards.repr} '
            '--enum_factor 0 '
            '--n_molecules 1 '
            '--min_tc 10 '
            '--sample_idx {wildcards.sample_idx} '
            '--rnn_type LSTM '
            '--embedding_size 128 '
            '--hidden_size 1024 '
            '--n_layers 3 '
            '--dropout 0 '
            '--batch_size 64 '
            '--learning_rate 0.001 '
            '--max_epochs 3 '
            '--patience 50000 '
            '--log_every_steps 100 '
            '--log_every_epochs 1 '
            '--sample_mols 500000 '
            '--input_file {input.input_file} '
            '--vocab_file {input.vocab_file} '
            '--smiles_file {output.smiles_file} '
            '--model_file {output.model_file} '
            '--loss_file {output.loss_file} '
            '--time_file {output.time_file} '
# -----------------------------------------------------------------------------
